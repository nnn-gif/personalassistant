[package]
name = "personalassistant"
version = "1.0.0"
edition = "2021"
default-run = "personalassistant"

[lib]
name = "personalassistant_lib"
crate-type = ["lib", "cdylib", "staticlib"]

[build-dependencies]
tauri-build = { version = "2", features = [] }

[dependencies]
tauri = { version = "2", features = [ "protocol-asset", "macos-private-api"] }
tauri-plugin-shell = "2"
tauri-plugin-dialog = "2"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
tokio = { version = "1", features = ["full"] }
tokio-stream = "0.1"
futures-util = "0.3"
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1", features = ["v4", "serde"] }
surrealdb = "2"
anyhow = "1"
thiserror = "2"
reqwest = { version = "0.12", features = ["json", "multipart"] }
scraper = "0.21"
genai = "0.3.5"
regex = "1"
url = "2"
urlencoding = "2"
tracing = "0.1"
tracing-subscriber = "0.3"
dirs = "5"
cpal = "0.15"
hound = "3.5"
byteorder = "1.5"
rubato = "0.14"
ringbuf = "0.3"
crossbeam-channel = "0.5"
sqlx = { version = "0.8", features = ["runtime-tokio-native-tls", "sqlite", "chrono", "uuid"] }
# Vector database
qdrant-client = "1.11"
# Document processing
lopdf = "0.30"
pdf-extract = "0.7"
zip = "2.1"
walkdir = "2.4"
mime_guess = "2.0"
# Local inference with Candle (moved to platform-specific sections below)
candle-nn = "0.9.1"
candle-transformers = "0.9.1"
tokenizers = { version = "0.20", features = ["http"] }
hf-hub = { version = "0.3", features = ["tokio"] }
rand = "0.8"
# Crane inference engine (built on Candle with better Metal support)
# crane-core = { git = "https://github.com/lucasjinreal/Crane.git", branch = "main" }
# Unified LLM interface for multiple backends
# callm = { version = "0.2", features = ["metal"] } # Temporarily disabled due to version conflicts
# Simplified approach - use system tools and enhanced Ollama
# vosk = "0.3.1"  # Commented out due to native library dependencies
# Configuration
toml = "0.8"
dotenv = "0.15"

# Platform-specific dependencies
[target.'cfg(target_os = "macos")'.dependencies]
# LLama.cpp with Metal support
llama_cpp = { version = "0.3", features = ["metal"] }
# Candle with Metal support
candle-core = { version = "0.9.1", features = ["metal"] }
# macOS system libraries
objc = "0.2"
objc-foundation = "0.1"
cocoa = "0.26"
core-graphics = "0.24"
core-foundation = "0.10"
objc2 = "0.5"
objc2-foundation = { version = "0.2", features = ["NSString", "NSDate", "NSArray", "NSBundle"] }
objc2-app-kit = { version = "0.2", features = ["NSWorkspace", "NSApplication"] }

[target.'cfg(target_os = "windows")'.dependencies]
# LLama.cpp with CUDA and Vulkan support
llama_cpp = { version = "0.3", features = ["cuda", "vulkan"] }
# Candle without Metal support
candle-core = "0.9.1"

[target.'cfg(not(any(target_os = "macos", target_os = "windows")))'.dependencies]
# LLama.cpp CPU-only for other platforms
llama_cpp = { version = "0.3" }
# Candle without Metal support
candle-core = "0.9.1"

[features]
default = ["custom-protocol"]
custom-protocol = ["tauri/custom-protocol"]

[[bin]]
name = "personalassistant"
path = "src/main.rs"

# Temporarily disabled test binaries to speed up build
# [[bin]]
# name = "test_crane_cpu"
# path = "test_crane_cpu.rs"

# [[bin]]
# name = "test_crane_metal_simple"
# path = "test_crane_metal_simple.rs"

# [[bin]]
# name = "test_bert_metal"
# path = "test_bert_metal.rs"

# [[bin]]
# name = "test_llama_cpp_metal"
# path = "test_llama_cpp_metal.rs"

# [[bin]]
# name = "test_llama_cpp_simple"
# path = "test_llama_cpp_simple.rs"
